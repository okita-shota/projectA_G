{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okita-shota/projectA_G/blob/main/YOLOv8_HumanPoseEstimation_%E3%81%AE10_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCDyng5FwCGO"
      },
      "source": [
        "# YOLOで人体姿勢推定\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HoO3xJLe0dnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Google Colab ngrok\n",
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "id": "wLhkCPQe1u7y",
        "outputId": "f800004e-d62e-4c19-ac96-f3a77d6e8f1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (3.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ngrok\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def hello_world():\n",
        "    return 'Hello, World!'\n",
        "\n"
      ],
      "metadata": {
        "id": "1rip-F2j2Ij2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flask\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8080)\n",
        "print('WebアプリのURL:', public_url)"
      ],
      "metadata": {
        "id": "jjETLt600f7N",
        "outputId": "aac028e1-1f7d-4765-f336-045124205c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2024-11-04T16:18:42+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-11-04T16:18:42+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-11-04T16:18:42+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2024-11-04T16:18:42+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9c7d450ee668>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8080\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WebアプリのURL:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y00. セットアップ"
      ],
      "metadata": {
        "id": "bjdNux7D6CFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WMDqk377sRqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7ce585-99fc-4c49-c3b0-0158b615eadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  2 15:45:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU9caVYlqm6e"
      },
      "source": [
        "## Y01. YOLOv8をインストール\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 YOLOv8をダウンロード\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "o0SSdLJwxIMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9d0a81-b058-40d8-b0b5-f7c8116bf050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.27 🚀 Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 32.1/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 必要なライブラリのインストール\n",
        "import csv\n",
        "import cv2\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "gZnHMnaN7JRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 学習モデルの読み込み。姿勢推論用のモデルデータを読み込む\n",
        "model = YOLO(\"yolov8n-pose.pt\")"
      ],
      "metadata": {
        "id": "JSfBs3yL7P4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0102be-42b7-4f55-e046-5d3c84b1d8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.52M/6.52M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 keypointの位置毎の名称定義\n",
        "KEYPOINTS_NAMES = [\n",
        "    \"nose\", \"eye(L)\", \"eye(R)\", \"ear(L)\", \"ear(R)\",\n",
        "    \"shoulder(L)\", \"shoulder(R)\", \"elbow(L)\", \"elbow(R)\",\n",
        "    \"wrist(L)\", \"wrist(R)\", \"hip(L)\", \"hip(R)\",\n",
        "    \"knee(L)\", \"knee(R)\", \"ankle(L)\", \"ankle(R)\"\n",
        "]"
      ],
      "metadata": {
        "id": "6TfiLevr7SnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 ビデオライターの設定\n",
        "def setup_video_writer(capture, output_path):\n",
        "    \"\"\"ビデオライターの設定\"\"\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return cv2.VideoWriter(output_path, fourcc, fps, (width, height))"
      ],
      "metadata": {
        "id": "dG0WE7kM73dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 姿勢情報をCSVファイルに書き出す\n",
        "def write_pose_to_csv(csv_path, frame_count, keypoints, confs):\n",
        "    \"\"\"姿勢情報をCSVファイルに書き出す\"\"\"\n",
        "    row = [frame_count]\n",
        "    for index, keypoint in enumerate(zip(keypoints, confs)):\n",
        "        x, y = int(keypoint[0][0]), int(keypoint[0][1])\n",
        "        score = keypoint[1]\n",
        "        row.extend([x, y, score])\n",
        "    with open(csv_path, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "3UN92Q7b8NoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 フレームにキーポイントと骨格を描画する\n",
        "def draw_keypoints(frame, keypoints, confs):\n",
        "    \"\"\"フレームにキーポイントと骨格を描画する\"\"\"\n",
        "    for index, keypoint in enumerate(zip(keypoints, confs)):\n",
        "        x, y = int(keypoint[0][0]), int(keypoint[0][1])\n",
        "        score = keypoint[1]\n",
        "        if score >= 0.5:\n",
        "            cv2.circle(frame, (x, y), 5, (255, 0, 255), -1)\n",
        "            cv2.putText(frame, KEYPOINTS_NAMES[index], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1, cv2.LINE_AA)\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "2L_xqf5M8QRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 動画をフレームごとに処理し、姿勢情報を取得してCSVと動画に保存する\n",
        "def process_video(input_video_path, output_video_path, csv_path):\n",
        "    \"\"\"動画をフレームごとに処理し、姿勢情報を取得してCSVと動画に保存する\"\"\"\n",
        "    capture = cv2.VideoCapture(input_video_path)\n",
        "    video_writer = setup_video_writer(capture, output_video_path)\n",
        "\n",
        "    with open(csv_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # ヘッダー行を書き込む\n",
        "        header = [\"frame\"]\n",
        "        for name in KEYPOINTS_NAMES:\n",
        "            header.extend([f\"{name}_x\", f\"{name}_y\", f\"{name}_score\"])\n",
        "        writer.writerow(header)\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while capture.isOpened():\n",
        "        success, frame = capture.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # 推論を実行\n",
        "        results = model(frame)\n",
        "\n",
        "        if len(results[0].keypoints) > 0:\n",
        "            keypoints = results[0].keypoints\n",
        "            confs = keypoints.conf[0].tolist()  # 推論結果:1に近いほど信頼度が高い\n",
        "            xys = keypoints.xy[0].tolist()  # 座標\n",
        "\n",
        "            # 姿勢情報をCSVファイルに書き出す\n",
        "            write_pose_to_csv(csv_path, frame_count, xys, confs)\n",
        "            # キーポイントと骨格をフレームに描画する\n",
        "            frame = draw_keypoints(frame, xys, confs)\n",
        "\n",
        "        # フレームに骨格情報を描画したものを動画に書き出す\n",
        "        video_writer.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    capture.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "DxFA_jLM8Sbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y02. サンプル動画（sample.mp4）の読み込み"
      ],
      "metadata": {
        "id": "yPGAi7FX2qwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Y02.1　（一番簡単）\n",
        "# ファイルをドラッグ＆ドロップ '/content/sample.mp4'\n",
        "\n",
        "# Y02-.2\n",
        "# 解析用動画のアップロード（無料アカウントの場合はエラー）\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Vut9AZ818-D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y03. サンプル動画（sample.mp4）の骨格情報の推定"
      ],
      "metadata": {
        "id": "PbQKLRMC9Jki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.1 （今回は）fpsを30で統一する"
      ],
      "metadata": {
        "id": "PlqkkyaJ79Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.1 今回はfpsを30で統一する\n",
        "\n",
        "# ドラッグ&ドロップの場合\n",
        "#!ffmpeg -y -i \"sample.mp4\" -vf \"fps=30\" \"sample_30fps.mp4\"\n",
        "\n",
        "# # Google Driveの場合\n",
        "!ffmpeg -y -i \"/content/drive/MyDrive//content//content/46894_640x360ストレッチ高齢者2.mp4/sample.mp4\" -vf \"fps=30\" \"sample_30fps.mp4\""
      ],
      "metadata": {
        "id": "V-_PFE5Vz9kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb3e155-9b9d-447a-c57e-8d3911ae9368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "\u001b[1;31m/content/drive/MyDrive//content//content/46894_640x360ストレッチ高齢者2.mp4/sample.mp4: No such file or directory\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik5xBt_Ek4Q3",
        "outputId": "9ee2d318-e65b-43c1-ccdf-7960957c8d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.2 動画のwidth/height/fpsを取得"
      ],
      "metadata": {
        "id": "N5RQ_xUe8C6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.2 動画のwidth/height/fpsを取得\n",
        "import cv2\n",
        "\n",
        "# ドラッグ&ドロップの場合\n",
        "# video_path = \"sample.mp4\"\n",
        "\n",
        "# # Google Driveの場合\n",
        "video_path = \"/content/drive/MyDrive/20240903_video/sample.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(\"sample.mp4\")\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "video_path = \"sample_30fps.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(\"\")\n",
        "print(\"sample_30fps.mp4\")\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))"
      ],
      "metadata": {
        "id": "iNJT5egu19Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af748ed-d3da-4e5b-fb44-f46ee5cc9428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample.mp4\n",
            "width:0.0, height:0.0, fps:0.0\n",
            "\n",
            "sample_30fps.mp4\n",
            "width:0.0, height:0.0, fps:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.3 入力動画のパスと出力ファイルのパスを指定"
      ],
      "metadata": {
        "id": "Xo2kNQcv8IeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.3 入力動画のパスと出力ファイルのパスを指定\n",
        "input_video_path = '/content/sample_30fps.mp4'\n",
        "output_video_path = 'sample_30fps_with_pose.mp4'\n",
        "csv_path = 'sample_30fps_pose_keypoints.csv'"
      ],
      "metadata": {
        "id": "TL0DfTqO0xDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.4 動画を処理し、姿勢情報を取得してCSVと動画に保存"
      ],
      "metadata": {
        "id": "LHTOcYnE8Q2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.4 動画を処理し、姿勢情報を取得してCSVと動画に保存\n",
        "process_video(input_video_path, output_video_path, csv_path)"
      ],
      "metadata": {
        "id": "NAc3TM2V0yvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.5 結果確認用動画の作成"
      ],
      "metadata": {
        "id": "OanfBW2V8T32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.5 結果確認用動画の作成\n",
        "!ffmpeg -y -i {output_video_path} -vf scale=600:-2 {\"sample_30fps_with_pose.mov\"}"
      ],
      "metadata": {
        "id": "wPHTcJO48WLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y03.6 動画の再生"
      ],
      "metadata": {
        "id": "2753ld_48XfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y03.6 動画の再生\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open( \"sample_30fps_with_pose.mov\", 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")\n"
      ],
      "metadata": {
        "id": "qgVM-zAV75Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y04. 比較用動画（f0.mp4）の読み込み"
      ],
      "metadata": {
        "id": "3pm6Jn8CZXJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Y04.1\n",
        "# ドラッグ＆ドロップ '/content/f0.mp4'\n",
        "\n",
        "# Y04.2\n",
        "# 解析したい動画のアップロード （無料アカウントの場合はエラー）\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "s7xGViJ4eTLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y05. 比較用動画（f0.mp4）の骨格情報の推定"
      ],
      "metadata": {
        "id": "UeQNZCjh24_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.1 （今回は）fpsを30で統一する"
      ],
      "metadata": {
        "id": "Ui_-gysF8eh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.1 今回はfpsを30で統一する\n",
        "# sample.mp4を変換した時と比べて、どこが変更されているか要確認。 答え：ファイル名だけです。\n",
        "# ドラッグ&ドロップの場合\n",
        "# !ffmpeg -y -i \"f0.mp4\" -vf \"fps=30\" \"f0_30fps.mp4\"\n",
        "\n",
        "# # Google Driveの場合\n",
        "!ffmpeg -y -i \"/content/drive/MyDrive/20240903_video/f0.mp4\" -vf \"fps=30\" \"f0_30fps.mp4\""
      ],
      "metadata": {
        "id": "tXvicZm8ZegN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.2 動画のwidth/height/fpsを取得"
      ],
      "metadata": {
        "id": "ZbQ6aqCY8i__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.2 動画のwidth/height/fpsを取得\n",
        "import cv2\n",
        "\n",
        "# # ドラッグ&ドロップの場合\n",
        "# video_path = \"f0.mp4\"\n",
        "# # Google Driveの場合\n",
        "video_path = \"/content/drive/MyDrive/20240903_video/f0.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(\"f0.mp4\")\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "video_path = \"f0_30fps.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(\"\")\n",
        "print(\"f0_30fps.mp4\")\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))"
      ],
      "metadata": {
        "id": "G37KLu1k6CPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.3 入力動画のパスと出力ファイルのパスを指定"
      ],
      "metadata": {
        "id": "KK5Bte-B8mI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.3 入力動画のパスと出力ファイルのパスを指定\n",
        "input_video_path = '/content/f0_30fps.mp4'\n",
        "output_video_path = 'f0_30fps_with_pose.mp4'\n",
        "csv_path = 'f0_30fps_pose_keypoints.csv'"
      ],
      "metadata": {
        "id": "FRtM6An23L8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.4 動画を処理し、姿勢情報を取得してCSVと動画に保存"
      ],
      "metadata": {
        "id": "_vyaNLDi83S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.4 動画を処理し、姿勢情報を取得してCSVと動画に保存\n",
        "process_video(input_video_path, output_video_path, csv_path)"
      ],
      "metadata": {
        "id": "VibjW1z0zJxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.5 結果確認用動画の作成"
      ],
      "metadata": {
        "id": "Q5E2dxqX86bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.5 結果確認用動画の作成\n",
        "!ffmpeg -y -i {output_video_path} -vf scale=600:-2 {\"f0_30fps_with_pose.mov\"}"
      ],
      "metadata": {
        "id": "l5EtDd623Szz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.6 動画の再生"
      ],
      "metadata": {
        "id": "5U4Z5DLJ89RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y05.6 動画の再生\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open( \"f0_30fps_with_pose.mov\", 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")\n"
      ],
      "metadata": {
        "id": "NGXyZ4okAGie",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y05.7 他の動画（f1.mp4）でも適用してみよう"
      ],
      "metadata": {
        "id": "UmDhSMGiH684"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Y05.7.1\n",
        "# f1.mp4をアップロードしよう"
      ],
      "metadata": {
        "id": "P94QCg7WIS9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y05.7.2\n",
        "# 簡単にするためにファイル名と拡張子を最初に定義\n",
        "file_name = \"f1\"\n",
        "extension = \".mp4\"\n",
        "\n",
        "### Y05.1 今回はfpsを30で統一する\n",
        "# # ドラッグ&ドロップの場合\n",
        "# !ffmpeg -y -i {file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "# # Google Driveの場合\n",
        "!ffmpeg -y -i \"/content/drive/MyDrive/20240903_video/\"{file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "\n",
        "### Y05.2 動画のwidth/height/fpsを取得\n",
        "import cv2\n",
        "\n",
        "video_path = file_name+extension\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(file_name+extension)\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "video_path = file_name+\"_30fps.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(\"\")\n",
        "print(file_name+\"_30fps.mp4\")\n",
        "print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "### Y05.3 入力動画のパスと出力ファイルのパスを指定\n",
        "input_video_path = \"/content/\"+file_name+\"_30fps.mp4\"\n",
        "output_video_path = file_name+\"_30fps_with_pose.mp4\"\n",
        "csv_path = file_name+\"_30fps_pose_keypoints.csv\"\n",
        "\n",
        "### Y05.4 動画を処理し、姿勢情報を取得してCSVと動画に保存\n",
        "process_video(input_video_path, output_video_path, csv_path)\n",
        "\n",
        "### Y05.5 結果確認用動画の作成\n",
        "!ffmpeg -y -i {output_video_path} -vf scale=600:-2 {file_name}_30fps_with_pose.mov\n",
        "\n",
        "### Y05.6 動画の再生\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open( file_name+\"_30fps_with_pose.mov\", 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N3I5QOHuIEnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y05.7.3\n",
        "# 関数として定義しておく\n",
        "\n",
        "def get_pose(file_name:str, extension:str):\n",
        "  ### Y05.1 今回はfpsを30で統一する\n",
        "  # # ドラッグ&ドロップの場合\n",
        "  # !ffmpeg -y -i {file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "  # # Google Driveの場合\n",
        "  !ffmpeg -y -i \"/content/drive/MyDrive/20240903_video/\"{file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "\n",
        "  ### Y05.2 動画のwidth/height/fpsを取得\n",
        "  import cv2\n",
        "\n",
        "  video_path = file_name+extension\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  print(file_name+extension)\n",
        "  print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "  video_path = file_name+\"_30fps.mp4\"\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  print(\"\")\n",
        "  print(file_name+\"_30fps.mp4\")\n",
        "  print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "  ### Y05.3 入力動画のパスと出力ファイルのパスを指定\n",
        "  input_video_path = \"/content/\"+file_name+\"_30fps.mp4\"\n",
        "  output_video_path = file_name+\"_30fps_with_pose.mp4\"\n",
        "  csv_path = file_name+\"_30fps_pose_keypoints.csv\"\n",
        "\n",
        "  ### Y05.4 動画を処理し、姿勢情報を取得してCSVと動画に保存\n",
        "  process_video(input_video_path, output_video_path, csv_path)\n",
        "\n",
        "  ### Y05.5 結果確認用動画の作成\n",
        "  !ffmpeg -y -i {output_video_path} -vf scale=600:-2 {file_name}_30fps_with_pose.mov\n",
        "\n",
        "  ### Y05.6 動画の再生\n",
        "  from IPython.display import HTML\n",
        "  from base64 import b64encode\n",
        "\n",
        "  mp4 = open( file_name+\"_30fps_with_pose.mov\", 'rb').read()\n",
        "  data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "  HTML(f\"\"\"\n",
        "  <video width=\"50%\" height=\"50%\" controls>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "  </video>\"\"\")"
      ],
      "metadata": {
        "id": "ndemouvCKtCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y05.7.4\n",
        "# f2.mp4をアップロードしてみよう"
      ],
      "metadata": {
        "id": "jaTle1z6LA7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y05.7.5\n",
        "# これ以降はこの関数を呼び出すだけで、骨格推定が実行できる\n",
        "get_pose(\"f2\", \".mp4\")"
      ],
      "metadata": {
        "id": "WK61DSlmLCm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y06. 単位ベクトルに変換"
      ],
      "metadata": {
        "id": "b3Z4n156z0_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.1 骨格情報が入ったcsvファイルの中身を確認"
      ],
      "metadata": {
        "id": "CDb65rmW9Eak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y06.1 骨格情報が入ったcsvファイルの中身を確認\n",
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"sample_30fps_pose_keypoints.csv\").head()"
      ],
      "metadata": {
        "id": "ee3xVzTkwjor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.2 0フレーム目を可視化\n"
      ],
      "metadata": {
        "id": "oMxFWfG_9Kp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y06.2 0フレーム目を可視化\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_zero_frame(file_path:str):\n",
        "  data = pd.read_csv(file_path)\n",
        "\n",
        "  # 0フレームのデータをフィルタリングする\n",
        "  frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "  # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "  keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "  # 0フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "  x_coords = frame_0_data.iloc[0, 1::3]\n",
        "  y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "  # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "  keypoints = {\n",
        "      'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "      'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "      'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "      'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "  }\n",
        "\n",
        "  connections = [\n",
        "      ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "      ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "      ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "      ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "      ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "      ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "  ]\n",
        "\n",
        "  # y軸を反転させた状態で0フレームの散布図をプロットする\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  # キーポイントをプロット\n",
        "  plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "  # 各キーポイントの横に名前をプロットする\n",
        "  for i, name in enumerate(keypoint_names):\n",
        "      plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "  # キーポイントを矢印で接続\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "  plt.xlabel('X Coordinates')\n",
        "  plt.ylabel('Y Coordinates')\n",
        "  plt.title('Scatter Plot of Keypoints for Frame 0')\n",
        "  plt.grid(True)\n",
        "  plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "  plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "DM68iuLk4urG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_zero_frame('sample_30fps_pose_keypoints.csv')"
      ],
      "metadata": {
        "id": "xz2BKg7NP2Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.3 0フレーム目を単位ベクトルに変換（可視化確認用）"
      ],
      "metadata": {
        "id": "q8WtTGPV_BgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_zero_frame_norm(file_path:str):\n",
        "  data = pd.read_csv(file_path)\n",
        "\n",
        "  # 0フレームのデータをフィルタリングする\n",
        "  frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "  # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "  keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "  # 0フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "  x_coords = frame_0_data.iloc[0, 1::3]\n",
        "  y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "  # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "  keypoints = {\n",
        "      'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "      'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "      'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "      'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "  }\n",
        "\n",
        "  connections = [\n",
        "      # ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "      ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "      ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "      ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "      ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "      ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "  ]\n",
        "\n",
        "  # y軸を反転させた状態で0フレームの散布図をプロットする\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  # キーポイントをプロット\n",
        "  plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "  # 各キーポイントの横に名前をプロットする\n",
        "  for i, name in enumerate(keypoint_names):\n",
        "      plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "  # キーポイントを矢印で接続\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "  # 単位ベクトルを計算する\n",
        "  unit_vectors = []\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      norm = np.sqrt(dx**2 + dy**2)\n",
        "      if norm != 0:\n",
        "          unit_vectors.append((dx / norm, dy / norm, x1, y1))\n",
        "\n",
        "  # 単位ベクトルをプロットする\n",
        "  for uv in unit_vectors:\n",
        "      plt.arrow(uv[2], uv[3], uv[0] * 100, uv[1] * 100, head_width=5, head_length=10, fc='red', ec='red', length_includes_head=True)\n",
        "\n",
        "  plt.xlabel('X Coordinates')\n",
        "  plt.ylabel('Y Coordinates')\n",
        "  plt.title('Visualization of Unit Vectors for Frame 0')\n",
        "  plt.grid(True)\n",
        "  plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "  plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "EK7PHHjV_BQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_zero_frame_norm( 'sample_30fps_pose_keypoints.csv')"
      ],
      "metadata": {
        "id": "ZSELWTbBQPlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.4 全てのフレームを単位ベクトルに変換してcsvファイルに格納"
      ],
      "metadata": {
        "id": "otxo176T_oBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y06.4 全てのフレームを単位ベクトルに変換してcsvファイルに格納\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 単位ベクトルを計算し、結果を連結する関数を定義\n",
        "def calc_and_concat_norms(cols, data):\n",
        "    unit_vectors_list = []\n",
        "    for col1, col2 in cols:\n",
        "        # 各点のconfidenceを取得\n",
        "        confidence_col1 = data[f'{col1}_score'].values\n",
        "        confidence_col2 = data[f'{col2}_score'].values\n",
        "\n",
        "        # ベクトルの差分を計算し、confidenceが0.1以下の場合は差分を0に設定\n",
        "        vector_diff = np.where((confidence_col1 > 0.1)[:, None] & (confidence_col2 > 0.1)[:, None],\n",
        "                               data[[f'{col2}_x', f'{col2}_y']].values - data[[f'{col1}_x', f'{col1}_y']].values,\n",
        "                               np.array([0, 0]))\n",
        "\n",
        "        # 単位ベクトルを計算\n",
        "        norm_diff = np.linalg.norm(vector_diff, axis=1, keepdims=True)\n",
        "        unit_vector_diff = np.where(norm_diff != 0, vector_diff / norm_diff, np.array([0, 0]))\n",
        "\n",
        "        # 結果をデータフレームに変換\n",
        "        unit_vectors_df = pd.DataFrame({\n",
        "            f'unit_x_{col1}_{col2}': unit_vector_diff[:, 0],\n",
        "            f'unit_y_{col1}_{col2}': unit_vector_diff[:, 1]\n",
        "        })\n",
        "\n",
        "        # 各ペアの結果をリストに追加\n",
        "        unit_vectors_list.append(unit_vectors_df)\n",
        "\n",
        "    # 全てのデータフレームを列方向に連結\n",
        "    final_df = pd.concat(unit_vectors_list, axis=1)\n",
        "    return final_df\n",
        "\n",
        "# 指定されたディレクトリ内のファイルを処理する関数を定義\n",
        "def transform_to_norm(filename: str):\n",
        "    # ファイルパスを設定\n",
        "    file_path = f\"{filename}_30fps_pose_keypoints.csv\"\n",
        "    output_file_path = f\"{filename}_30fps_pose_keypoints_norm.csv\"\n",
        "\n",
        "    # CSVファイルを読み込む\n",
        "    data = pd.read_csv(file_path).fillna(0)\n",
        "\n",
        "    # 対象の点のペアを定義（0フレーム目の可視化に基づく）\n",
        "    points_pairs = [\n",
        "        # ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "        ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "        ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "        ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "        ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "        ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "    ]\n",
        "\n",
        "    # 単位ベクトルを計算し連結\n",
        "    result_df = calc_and_concat_norms(points_pairs, data)\n",
        "\n",
        "    # 結果をCSVファイルに保存\n",
        "    result_df.to_csv(output_file_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "QzN9D2Mx_n1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定されたディレクトリで関数を実行\n",
        "transform_to_norm(\"sample\")\n",
        "transform_to_norm(\"f0\")\n",
        "transform_to_norm(\"f1\")\n",
        "transform_to_norm(\"f2\")"
      ],
      "metadata": {
        "id": "NTQjVn66KAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.5 変換結果のcsvファイルの中身を確認"
      ],
      "metadata": {
        "id": "bCDghgWxB7pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y06.5 変換結果のcsvファイルの中身を確認\n",
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"sample_30fps_pose_keypoints_norm.csv\").head()"
      ],
      "metadata": {
        "id": "nZ_lzwxzB7fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y06.5 変換結果の可視化"
      ],
      "metadata": {
        "id": "1UciqeEMBJbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y06.5 変換結果の可視化\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def visualize_norms(file_name: str):\n",
        "    # CSVファイルを読み込む\n",
        "    file_path = file_name + '_30fps_pose_keypoints.csv'\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 単位ベクトルが計算された結果のCSVファイルを読み込む\n",
        "    norm_file_path = file_name + '_30fps_pose_keypoints_norm.csv'\n",
        "    norm_data = pd.read_csv(norm_file_path)\n",
        "\n",
        "    # フレーム数を取得\n",
        "    num_frames = norm_data.shape[0]\n",
        "\n",
        "    # カラーマップを設定（半透明にする）\n",
        "    colors = cm.viridis(np.linspace(0, 1, num_frames))\n",
        "    colors[:, 3] = 0.2  # アルファ値を設定（0.5にする）\n",
        "\n",
        "    # 初期フレームのデータをフィルタリングする\n",
        "    frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "    # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "    keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "    # 初期フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "    x_coords = frame_0_data.iloc[0, 1::3]\n",
        "    y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "    # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "    keypoints = {\n",
        "        'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "        'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "        'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "        'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "    }\n",
        "\n",
        "    connections = [\n",
        "        ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "        ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "        ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "        ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "        ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "    ]\n",
        "\n",
        "    # y軸を反転させた状態で初期フレームの散布図をプロットする\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # 初期フレームのキーポイントをプロット\n",
        "    plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "    # 各キーポイントの横に名前をプロットする\n",
        "    for i, name in enumerate(keypoint_names):\n",
        "        plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "    # 初期フレームのキーポイントを矢印で接続\n",
        "    for connection in connections:\n",
        "        x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "        x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "        dx, dy = x2 - x1, y2 - y1\n",
        "        plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "    # 単位ベクトルをプロットする\n",
        "    for connection in connections:\n",
        "        unit_x_col = f'unit_x_{connection[0]}_{connection[1]}'\n",
        "        unit_y_col = f'unit_y_{connection[0]}_{connection[1]}'\n",
        "        if unit_x_col in norm_data.columns and unit_y_col in norm_data.columns:\n",
        "            for i in range(num_frames):\n",
        "                x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "                unit_x, unit_y = norm_data.iloc[i][unit_x_col], norm_data.iloc[i][unit_y_col]\n",
        "                plt.arrow(x1, y1, unit_x * 100, unit_y * 100, head_width=5, head_length=10, fc=colors[i], ec=colors[i], length_includes_head=True)\n",
        "\n",
        "    plt.xlabel('X Coordinates')\n",
        "    plt.ylabel('Y Coordinates')\n",
        "    plt.title('Visualization of Unit Vectors Over Time')\n",
        "    plt.grid(True)\n",
        "    plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "    plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0YrLpS0M-GBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_norms(\"sample\")"
      ],
      "metadata": {
        "id": "yqidcAK5PeZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Y07. 動的時間伸縮法 / DTW (Dynamic Time Warping) を計算"
      ],
      "metadata": {
        "id": "H0rCZzAMFuFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.gyazo.com/7cd1e5f76b8dd0f8db21792cfd09f517.gif\" alt=\"GIF Image\">\n"
      ],
      "metadata": {
        "id": "epPwbBwRGAy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y07.1 ライブラリをインストール\n",
        "!pip install fastdtw"
      ],
      "metadata": {
        "id": "FIe7OW0wAeSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Y07.2 DTWの計算\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "def calc_dtw(a, b):\n",
        "  x = np.array(pd.read_csv(a).fillna(0))\n",
        "  y = np.array(pd.read_csv(b).fillna(0))\n",
        "  distance, path = fastdtw(x, y, dist=euclidean)\n",
        "  print(a, b, distance)\n",
        " # return distance\n",
        "calc_dtw(\"sample_30fps_pose_keypoints_norm.csv\",\"f0_30fps_pose_keypoints_norm.csv\")\n",
        "calc_dtw(\"sample_30fps_pose_keypoints_norm.csv\",\"f1_30fps_pose_keypoints_norm.csv\")\n",
        "calc_dtw(\"sample_30fps_pose_keypoints_norm.csv\",\"f2_30fps_pose_keypoints_norm.csv\")"
      ],
      "metadata": {
        "id": "-kW3N0o9AeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DTWは時間軸方向を圧縮しているため、リズムに合わせるなどは無視してしまっている点に注意！"
      ],
      "metadata": {
        "id": "6Mj1o41PG0tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Y08. 二乗平均平方根誤差 / RMSE(Root Mean Squared Error)を計算する"
      ],
      "metadata": {
        "id": "kpYwv0LCGlcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://tsuchidalab.jp/wp1/wp-content/uploads/2024/05/1000.png\" alt=\"Image\">\n"
      ],
      "metadata": {
        "id": "qwRRA0DCG23g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Y08.2 Root Mean Squared Error(RMSE)の計算\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calc_rmse(a, b):\n",
        "  x = pd.read_csv(a).fillna(0)\n",
        "  y = pd.read_csv(b).fillna(0)[:len(x)] #元動画の長さに合わせる\n",
        "  rmse = np.sqrt(mean_squared_error(x, y))\n",
        "  print(a, b,'{:.3f}'.format(rmse))\n",
        "\n",
        "calc_rmse(\"sample_30fps_pose_keypoints_norm.csv\",\"f0_30fps_pose_keypoints_norm.csv\")\n",
        "calc_rmse(\"sample_30fps_pose_keypoints_norm.csv\",\"f1_30fps_pose_keypoints_norm.csv\")\n",
        "calc_rmse(\"sample_30fps_pose_keypoints_norm.csv\",\"f2_30fps_pose_keypoints_norm.csv\")"
      ],
      "metadata": {
        "id": "yq1E__vDGmgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "厳密にチェックしているけど、ちょっとでもリズムを取るタイミングがズレると、動きが似ていても大きな値を取ってしまう"
      ],
      "metadata": {
        "id": "J3XkP585G6NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Y09 オリジナル動画を読み込んでみよう"
      ],
      "metadata": {
        "id": "NOq6LK2DQbjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## オリジナル一つ目の動画をアップロード"
      ],
      "metadata": {
        "id": "QQud3_0nTcjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 動画の長さは以下のコマンドを使って適宜編集しよう\n",
        "# スマホ上で編集するのが一番楽だとは思います（注：研究評価など厳密なスコアを算出する際には楽曲ベースで正確に切り出すこと）\n",
        "# 動画の縦横は気にしなくて良いです\n",
        "\n",
        "uploadfile = \"test.mov\"\n",
        "# 開始10秒後から30秒間の動画を切り出したい場合、次のようにコマンドを入力\n",
        "# !ffmpeg -ss 10 -i {uploadfile} -t 30 -c copy test.mp4"
      ],
      "metadata": {
        "id": "vY-SGE4oSjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 動画は\"/content/drive/MyDrive/20240903_video/\" にアップロードすること。じゃないと動きません（パスの話）\n",
        "\n",
        "### 骨格推定の適用\n",
        "fname = \"test\" # ここの名前をアップロードした名前で。楽なのはこれに合わせてtestという名前の動画にしておく。\n",
        "\n",
        "# ファイル名は適宜アップロードした内容と合わせること\n",
        "get_pose(fname, \".mp4\") # ここの拡張子もアップロードした名前で。iPhoneならMOVになると思います。\n",
        "\n",
        "### 推定結果の確認\n",
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(fname + \"_30fps_pose_keypoints.csv\").head()\n",
        "\n",
        "# ゼロフレーム目の可視化\n",
        "visualize_zero_frame(fname + '_30fps_pose_keypoints.csv')\n",
        "\n",
        "# ゼロフレーム目の単位ベクトルの可視化\n",
        "visualize_zero_frame_norm(fname + '_30fps_pose_keypoints.csv')\n",
        "\n",
        "# 単位ベクトル作成\n",
        "transform_to_norm(fname)\n",
        "\n",
        "# 単位ベクトルの可視化\n",
        "visualize_norms(fname)"
      ],
      "metadata": {
        "id": "vO87mCeZP8fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 課題動画"
      ],
      "metadata": {
        "id": "BjniaZf7TYmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 骨格推定の適用\n",
        "fname = \"mihon\"\n",
        "\n",
        "# ファイル名は適宜アップロードした内容と合わせること\n",
        "get_pose(fname, \".mp4\")\n",
        "\n",
        "### 推定結果の確認\n",
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(fname + \"_30fps_pose_keypoints.csv\").head()\n",
        "\n",
        "# ゼロフレーム目の可視化\n",
        "visualize_zero_frame(fname + '_30fps_pose_keypoints.csv')\n",
        "\n",
        "# ゼロフレーム目の単位ベクトルの可視化\n",
        "visualize_zero_frame_norm(fname + '_30fps_pose_keypoints.csv')\n",
        "\n",
        "# 単位ベクトル作成\n",
        "transform_to_norm(fname)\n",
        "\n",
        "# 単位ベクトルの可視化\n",
        "visualize_norms(fname)"
      ],
      "metadata": {
        "id": "a9b7vq_NP8cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DTWコスト計算"
      ],
      "metadata": {
        "id": "dTBN4YGETgHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DTWコスト計算\n",
        "\n",
        "calc_dtw(\"test_30fps_pose_keypoints_norm.csv\",\"mihon_30fps_pose_keypoints_norm.csv\")"
      ],
      "metadata": {
        "id": "FO6jVJ55RJU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oXjouxgmNCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds17rbS3mMy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以下はdef関連のエラーが出た人用。以下のコードを実行後に上の課題用コードに戻ってください。"
      ],
      "metadata": {
        "id": "3qWJVl6KmNvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 YOLOv8をダウンロード\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "# 2 必要なライブラリのインストール\n",
        "import csv\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# 3 学習モデルの読み込み。姿勢推論用のモデルデータを読み込む\n",
        "model = YOLO(\"yolov8n-pose.pt\")\n",
        "\n",
        "# 4 keypointの位置毎の名称定義\n",
        "KEYPOINTS_NAMES = [\n",
        "    \"nose\", \"eye(L)\", \"eye(R)\", \"ear(L)\", \"ear(R)\",\n",
        "    \"shoulder(L)\", \"shoulder(R)\", \"elbow(L)\", \"elbow(R)\",\n",
        "    \"wrist(L)\", \"wrist(R)\", \"hip(L)\", \"hip(R)\",\n",
        "    \"knee(L)\", \"knee(R)\", \"ankle(L)\", \"ankle(R)\"\n",
        "]\n",
        "\n",
        "# 5 ビデオライターの設定\n",
        "def setup_video_writer(capture, output_path):\n",
        "    \"\"\"ビデオライターの設定\"\"\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# 6 姿勢情報をCSVファイルに書き出す\n",
        "def write_pose_to_csv(csv_path, frame_count, keypoints, confs):\n",
        "    \"\"\"姿勢情報をCSVファイルに書き出す\"\"\"\n",
        "    row = [frame_count]\n",
        "    for index, keypoint in enumerate(zip(keypoints, confs)):\n",
        "        x, y = int(keypoint[0][0]), int(keypoint[0][1])\n",
        "        score = keypoint[1]\n",
        "        row.extend([x, y, score])\n",
        "    with open(csv_path, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(row)\n",
        "\n",
        "# 7 フレームにキーポイントと骨格を描画する\n",
        "def draw_keypoints(frame, keypoints, confs):\n",
        "    \"\"\"フレームにキーポイントと骨格を描画する\"\"\"\n",
        "    for index, keypoint in enumerate(zip(keypoints, confs)):\n",
        "        x, y = int(keypoint[0][0]), int(keypoint[0][1])\n",
        "        score = keypoint[1]\n",
        "        if score >= 0.5:\n",
        "            cv2.circle(frame, (x, y), 5, (255, 0, 255), -1)\n",
        "            cv2.putText(frame, KEYPOINTS_NAMES[index], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1, cv2.LINE_AA)\n",
        "    return frame\n",
        "\n",
        "# 8 動画をフレームごとに処理し、姿勢情報を取得してCSVと動画に保存する\n",
        "def process_video(input_video_path, output_video_path, csv_path):\n",
        "    \"\"\"動画をフレームごとに処理し、姿勢情報を取得してCSVと動画に保存する\"\"\"\n",
        "    capture = cv2.VideoCapture(input_video_path)\n",
        "    video_writer = setup_video_writer(capture, output_video_path)\n",
        "\n",
        "    with open(csv_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # ヘッダー行を書き込む\n",
        "        header = [\"frame\"]\n",
        "        for name in KEYPOINTS_NAMES:\n",
        "            header.extend([f\"{name}_x\", f\"{name}_y\", f\"{name}_score\"])\n",
        "        writer.writerow(header)\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while capture.isOpened():\n",
        "        success, frame = capture.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # 推論を実行\n",
        "        results = model(frame)\n",
        "\n",
        "        if len(results[0].keypoints) > 0:\n",
        "            keypoints = results[0].keypoints\n",
        "            confs = keypoints.conf[0].tolist()  # 推論結果:1に近いほど信頼度が高い\n",
        "            xys = keypoints.xy[0].tolist()  # 座標\n",
        "\n",
        "            # 姿勢情報をCSVファイルに書き出す\n",
        "            write_pose_to_csv(csv_path, frame_count, xys, confs)\n",
        "            # キーポイントと骨格をフレームに描画する\n",
        "            frame = draw_keypoints(frame, xys, confs)\n",
        "\n",
        "        # フレームに骨格情報を描画したものを動画に書き出す\n",
        "        video_writer.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    capture.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Y05.7.3\n",
        "# 関数として定義しておく\n",
        "\n",
        "def get_pose(file_name:str, extension:str):\n",
        "  ### Y05.1 今回はfpsを30で統一する\n",
        "  # # ドラッグ&ドロップの場合\n",
        "  # !ffmpeg -y -i {file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "  # # Google Driveの場合\n",
        "  !ffmpeg -y -i \"/content/drive/MyDrive/20240903_video/\"{file_name}{extension} -vf \"fps=30\" {file_name}\"_30fps.mp4\"\n",
        "\n",
        "  ### Y05.2 動画のwidth/height/fpsを取得\n",
        "  import cv2\n",
        "\n",
        "  video_path = file_name+extension\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  print(file_name+extension)\n",
        "  print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "  video_path = file_name+\"_30fps.mp4\"\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  print(\"\")\n",
        "  print(file_name+\"_30fps.mp4\")\n",
        "  print(\"width:{}, height:{}, fps:{}\".format(width,height,fps))\n",
        "\n",
        "  ### Y05.3 入力動画のパスと出力ファイルのパスを指定\n",
        "  input_video_path = \"/content/\"+file_name+\"_30fps.mp4\"\n",
        "  output_video_path = file_name+\"_30fps_with_pose.mp4\"\n",
        "  csv_path = file_name+\"_30fps_pose_keypoints.csv\"\n",
        "\n",
        "  ### Y05.4 動画を処理し、姿勢情報を取得してCSVと動画に保存\n",
        "  process_video(input_video_path, output_video_path, csv_path)\n",
        "\n",
        "  ### Y05.5 結果確認用動画の作成\n",
        "  !ffmpeg -y -i {output_video_path} -vf scale=600:-2 {file_name}_30fps_with_pose.mov\n",
        "\n",
        "  ### Y05.6 動画の再生\n",
        "  from IPython.display import HTML\n",
        "  from base64 import b64encode\n",
        "\n",
        "  mp4 = open( file_name+\"_30fps_with_pose.mov\", 'rb').read()\n",
        "  data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "  HTML(f\"\"\"\n",
        "  <video width=\"50%\" height=\"50%\" controls>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "  </video>\"\"\")\n",
        "\n",
        "### Y06.2 0フレーム目を可視化\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_zero_frame(file_path:str):\n",
        "  data = pd.read_csv(file_path)\n",
        "\n",
        "  # 0フレームのデータをフィルタリングする\n",
        "  frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "  # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "  keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "  # 0フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "  x_coords = frame_0_data.iloc[0, 1::3]\n",
        "  y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "  # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "  keypoints = {\n",
        "      'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "      'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "      'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "      'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "  }\n",
        "\n",
        "  connections = [\n",
        "      ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "      ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "      ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "      ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "      ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "      ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "  ]\n",
        "\n",
        "  # y軸を反転させた状態で0フレームの散布図をプロットする\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  # キーポイントをプロット\n",
        "  plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "  # 各キーポイントの横に名前をプロットする\n",
        "  for i, name in enumerate(keypoint_names):\n",
        "      plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "  # キーポイントを矢印で接続\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "  plt.xlabel('X Coordinates')\n",
        "  plt.ylabel('Y Coordinates')\n",
        "  plt.title('Scatter Plot of Keypoints for Frame 0')\n",
        "  plt.grid(True)\n",
        "  plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "  plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "  plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_zero_frame_norm(file_path:str):\n",
        "  data = pd.read_csv(file_path)\n",
        "\n",
        "  # 0フレームのデータをフィルタリングする\n",
        "  frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "  # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "  keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "  # 0フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "  x_coords = frame_0_data.iloc[0, 1::3]\n",
        "  y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "  # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "  keypoints = {\n",
        "      'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "      'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "      'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "      'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "  }\n",
        "\n",
        "  connections = [\n",
        "      # ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "      ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "      ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "      ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "      ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "      ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "  ]\n",
        "\n",
        "  # y軸を反転させた状態で0フレームの散布図をプロットする\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  # キーポイントをプロット\n",
        "  plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "  # 各キーポイントの横に名前をプロットする\n",
        "  for i, name in enumerate(keypoint_names):\n",
        "      plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "  # キーポイントを矢印で接続\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "  # 単位ベクトルを計算する\n",
        "  unit_vectors = []\n",
        "  for connection in connections:\n",
        "      x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "      x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "      dx, dy = x2 - x1, y2 - y1\n",
        "      norm = np.sqrt(dx**2 + dy**2)\n",
        "      if norm != 0:\n",
        "          unit_vectors.append((dx / norm, dy / norm, x1, y1))\n",
        "\n",
        "  # 単位ベクトルをプロットする\n",
        "  for uv in unit_vectors:\n",
        "      plt.arrow(uv[2], uv[3], uv[0] * 100, uv[1] * 100, head_width=5, head_length=10, fc='red', ec='red', length_includes_head=True)\n",
        "\n",
        "  plt.xlabel('X Coordinates')\n",
        "  plt.ylabel('Y Coordinates')\n",
        "  plt.title('Visualization of Unit Vectors for Frame 0')\n",
        "  plt.grid(True)\n",
        "  plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "  plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "  plt.show()\n",
        "\n",
        "### Y06.4 全てのフレームを単位ベクトルに変換してcsvファイルに格納\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 単位ベクトルを計算し、結果を連結する関数を定義\n",
        "def calc_and_concat_norms(cols, data):\n",
        "    unit_vectors_list = []\n",
        "    for col1, col2 in cols:\n",
        "        # 各点のconfidenceを取得\n",
        "        confidence_col1 = data[f'{col1}_score'].values\n",
        "        confidence_col2 = data[f'{col2}_score'].values\n",
        "\n",
        "        # ベクトルの差分を計算し、confidenceが0.1以下の場合は差分を0に設定\n",
        "        vector_diff = np.where((confidence_col1 > 0.1)[:, None] & (confidence_col2 > 0.1)[:, None],\n",
        "                               data[[f'{col2}_x', f'{col2}_y']].values - data[[f'{col1}_x', f'{col1}_y']].values,\n",
        "                               np.array([0, 0]))\n",
        "\n",
        "        # 単位ベクトルを計算\n",
        "        norm_diff = np.linalg.norm(vector_diff, axis=1, keepdims=True)\n",
        "        unit_vector_diff = np.where(norm_diff != 0, vector_diff / norm_diff, np.array([0, 0]))\n",
        "\n",
        "        # 結果をデータフレームに変換\n",
        "        unit_vectors_df = pd.DataFrame({\n",
        "            f'unit_x_{col1}_{col2}': unit_vector_diff[:, 0],\n",
        "            f'unit_y_{col1}_{col2}': unit_vector_diff[:, 1]\n",
        "        })\n",
        "\n",
        "        # 各ペアの結果をリストに追加\n",
        "        unit_vectors_list.append(unit_vectors_df)\n",
        "\n",
        "    # 全てのデータフレームを列方向に連結\n",
        "    final_df = pd.concat(unit_vectors_list, axis=1)\n",
        "    return final_df\n",
        "\n",
        "# 指定されたディレクトリ内のファイルを処理する関数を定義\n",
        "def transform_to_norm(filename: str):\n",
        "    # ファイルパスを設定\n",
        "    file_path = f\"{filename}_30fps_pose_keypoints.csv\"\n",
        "    output_file_path = f\"{filename}_30fps_pose_keypoints_norm.csv\"\n",
        "\n",
        "    # CSVファイルを読み込む\n",
        "    data = pd.read_csv(file_path).fillna(0)\n",
        "\n",
        "    # 対象の点のペアを定義（0フレーム目の可視化に基づく）\n",
        "    points_pairs = [\n",
        "        # ('nose', 'eye(L)'), ('nose', 'eye(R)'), ('eye(L)', 'ear(L)'), ('eye(R)', 'ear(R)'),\n",
        "        ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "        ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "        ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "        ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "        ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "    ]\n",
        "\n",
        "    # 単位ベクトルを計算し連結\n",
        "    result_df = calc_and_concat_norms(points_pairs, data)\n",
        "\n",
        "    # 結果をCSVファイルに保存\n",
        "    result_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "\n",
        "### Y06.5 変換結果の可視化\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def visualize_norms(file_name: str):\n",
        "    # CSVファイルを読み込む\n",
        "    file_path = file_name + '_30fps_pose_keypoints.csv'\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # 単位ベクトルが計算された結果のCSVファイルを読み込む\n",
        "    norm_file_path = file_name + '_30fps_pose_keypoints_norm.csv'\n",
        "    norm_data = pd.read_csv(norm_file_path)\n",
        "\n",
        "    # フレーム数を取得\n",
        "    num_frames = norm_data.shape[0]\n",
        "\n",
        "    # カラーマップを設定（半透明にする）\n",
        "    colors = cm.viridis(np.linspace(0, 1, num_frames))\n",
        "    colors[:, 3] = 0.2  # アルファ値を設定（0.5にする）\n",
        "\n",
        "    # 初期フレームのデータをフィルタリングする\n",
        "    frame_0_data = data[data['frame'] == 0]\n",
        "\n",
        "    # キーポイントの名前のリスト（列名からスコアを除いたもの）\n",
        "    keypoint_names = [col.split('_')[0] for col in frame_0_data.columns[1::3]]\n",
        "\n",
        "    # 初期フレームのすべてのキーポイントのx座標とy座標を抽出する\n",
        "    x_coords = frame_0_data.iloc[0, 1::3]\n",
        "    y_coords = frame_0_data.iloc[0, 2::3]\n",
        "\n",
        "    # キーポイントの接続を定義（CSVのラベルに基づく）\n",
        "    keypoints = {\n",
        "        'nose': 0, 'eye(L)': 1, 'eye(R)': 2, 'ear(L)': 3, 'ear(R)': 4,\n",
        "        'shoulder(L)': 5, 'shoulder(R)': 6, 'elbow(L)': 7, 'elbow(R)': 8,\n",
        "        'wrist(L)': 9, 'wrist(R)': 10, 'hip(L)': 11, 'hip(R)': 12,\n",
        "        'knee(L)': 13, 'knee(R)': 14, 'ankle(L)': 15, 'ankle(R)': 16\n",
        "    }\n",
        "\n",
        "    connections = [\n",
        "        ('nose', 'shoulder(L)'), ('nose', 'shoulder(R)'), ('shoulder(L)', 'elbow(L)'),\n",
        "        ('shoulder(R)', 'elbow(R)'), ('elbow(L)', 'wrist(L)'), ('elbow(R)', 'wrist(R)'),\n",
        "        ('shoulder(L)', 'shoulder(R)'), ('shoulder(L)', 'hip(L)'), ('shoulder(R)', 'hip(R)'),\n",
        "        ('hip(L)', 'hip(R)'), ('hip(L)', 'knee(L)'), ('hip(R)', 'knee(R)'),\n",
        "        ('knee(L)', 'ankle(L)'), ('knee(R)', 'ankle(R)')\n",
        "    ]\n",
        "\n",
        "    # y軸を反転させた状態で初期フレームの散布図をプロットする\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # 初期フレームのキーポイントをプロット\n",
        "    plt.scatter(x_coords, y_coords, c='blue')\n",
        "\n",
        "    # 各キーポイントの横に名前をプロットする\n",
        "    for i, name in enumerate(keypoint_names):\n",
        "        plt.text(x_coords[i], y_coords[i], name, fontsize=9, ha='right')\n",
        "\n",
        "    # 初期フレームのキーポイントを矢印で接続\n",
        "    for connection in connections:\n",
        "        x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "        x2, y2 = x_coords[keypoints[connection[1]]], y_coords[keypoints[connection[1]]]\n",
        "        dx, dy = x2 - x1, y2 - y1\n",
        "        plt.arrow(x1, y1, dx, dy, head_width=15, head_length=25, fc='gray', ec='gray', length_includes_head=True)\n",
        "\n",
        "    # 単位ベクトルをプロットする\n",
        "    for connection in connections:\n",
        "        unit_x_col = f'unit_x_{connection[0]}_{connection[1]}'\n",
        "        unit_y_col = f'unit_y_{connection[0]}_{connection[1]}'\n",
        "        if unit_x_col in norm_data.columns and unit_y_col in norm_data.columns:\n",
        "            for i in range(num_frames):\n",
        "                x1, y1 = x_coords[keypoints[connection[0]]], y_coords[keypoints[connection[0]]]\n",
        "                unit_x, unit_y = norm_data.iloc[i][unit_x_col], norm_data.iloc[i][unit_y_col]\n",
        "                plt.arrow(x1, y1, unit_x * 100, unit_y * 100, head_width=5, head_length=10, fc=colors[i], ec=colors[i], length_includes_head=True)\n",
        "\n",
        "    plt.xlabel('X Coordinates')\n",
        "    plt.ylabel('Y Coordinates')\n",
        "    plt.title('Visualization of Unit Vectors Over Time')\n",
        "    plt.grid(True)\n",
        "    plt.gca().invert_yaxis()  # 原点を左上にするためにy軸を反転する\n",
        "    plt.axis('equal')  # 縦軸と横軸のスケールを同じにする\n",
        "    plt.show()\n",
        "\n",
        "### Y07.1 ライブラリをインストール\n",
        "!pip install fastdtw\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "\n",
        "def calc_dtw(a, b):\n",
        "  x = np.array(pd.read_csv(a).fillna(0))\n",
        "  y = np.array(pd.read_csv(b).fillna(0))\n",
        "  distance, path = fastdtw(x, y, dist=euclidean)\n",
        "  print(a, b, distance)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calc_rmse(a, b):\n",
        "  x = pd.read_csv(a).fillna(0)\n",
        "  y = pd.read_csv(b).fillna(0)[:len(x)] #元動画の長さに合わせる\n",
        "  rmse = np.sqrt(mean_squared_error(x, y))\n",
        "  print(a, b,'{:.3f}'.format(rmse))"
      ],
      "metadata": {
        "id": "PBtH1jK-mQRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}